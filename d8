                            *** Specialized Sorting
      
//Merge Sort:

Worst-case time complexity: O(n log n)
Best-case time complexity: O(n log n)
Average-case time complexity: O(n log n)
Merge Sort is a divide-and-conquer algorithm that divides the unsorted list into n sublists, each containing one element, and then repeatedly merges sublists to produce new sorted sublists.
code:-
Input: [5, 2, 9, 3, 4]
Merge Sort: [2, 3, 4, 5, 9]
def mergesort(arr):
    if len(arr)<=1:
        return arr
    else:
        mid=len(arr)//2
        left_half=arr[:mid]
        right_half=arr[mid:]
        left_half=mergesort(left_half)
        right_half=mergesort(right_half)
        merged=merge(left_half,right_half)
        return merged
def merge(left,right):
    result=[]
    i,j=0,0
    while i<len(left) and j<len(right):
        if left[i]<right[j]:
            result.append(left[i])
            i+=1
        else:
            result.append(right[j])
            j+=1
        result.extend(left[i:])
        result.extend(right[j:])
        return result
arr=[34,78,56,71,91,12,9]
sorted_arr=mergesort(arr)
print(sorted_arr)

output:-     
[9, 34, 56, 78, 71, 91, 12]
----------------------------------------------------------------------------------------------------
Quick Sort:

Worst-case time complexity: O(n^2) (rare, with poor pivot choice)
Best-case time complexity: O(n log n)
Average-case time complexity: O(n log n)
Quick Sort chooses a "pivot" element and partitions the array into two sub-arrays, one containing elements less than the pivot and the other containing elements greater than the pivot. It then recursively sorts the sub-arrays.

Input: [5, 2, 9, 3, 4]
Quick Sort: [2, 3, 4, 5, 9]
code:-
def quick_sort(arr):
    if len(arr) <= 1:
        return arr
        
    pivot = arr[0]
    left = [i for i in arr if i < pivot]
    right = [i for i in arr if i > pivot]
    return quick_sort(left)+[pivot]+quick_sort(right)
arr = [31,29,23,87,36,49]
print(arr)
result = quick_sort(arr)
print(result)
----------------------------------------------------------------------------------------------------

The order of sorting algorithms based on their average-case time complexity from fastest to slowest is as follows:

Quick Sort: O(n log n) - Fastest in practice for most data sets due to its good average-case performance.
Merge Sort: O(n log n) - Stable and consistent performance across different data sets.
Heap Sort: O(n log n) - Not as commonly used as Quick Sort or Merge Sort in practice but still efficient.
Insertion Sort: O(n^2) - Efficient for small data sets, but becomes slow for large ones.
Selection Sort: O(n^2) - Simple but not very efficient, especially for large data sets.
Bubble Sort: O(n^2) - One of the slowest sorting algorithms and rarely used for large data sets.

----------------------------------------------------------------------------------------------------
#count sort
-------------------------------------------------------------------------------------------------
l=list(map(int,input().split(',')))
count = [0 for i in range(10)]
for i in range(len(l)):
    count[l[i]] += 1
for i in range(1, len(count)):
    count[i] += count[i - 1]
res = [0] * len(l)
for i in range(len(l)):
    res[count[l[i]] - 1] = l[i]
    count[l[i]] -= 1

print(count)
print(res)
//output:-1,5,8,2,3
[0, 0, 1, 2, 3, 3, 4, 4, 4, 5]
[1, 2, 3, 5, 8]
-------------------------------------------------------------------------------------------------
#Simplified Master's Theorem
-------------------------------------------------------------------------------------------------
t(n)=at(n/b)+f(n)
      |        |
     Inside   outside
        Recursion
B->factor by which problem statement should be divided
    Eg:-for binary by 2
a->no of recursions (or) recurrence relations
f(n)->work done outside recursion
CONDITION:-
          a>=1 and b>1
NOTE:- to find height we consider b ->n/b^k=1 ->n=b^k
        Logn=logb^k
        Logn= klogb
--------       
|logn=k|
--------
* if b<1 then height=n
 ---->to find width. We consider a (I.e no of leaf nodes)
     =a^k
     =a^log n=n^log a=
           b.      b
Proof:-
     Take log both side
---------------------------------------------------------------------------------------
 The Master Theorem provides a way to analyze the time complexity of divide-and-conquer algorithms. It has three cases, each with a specific condition:

Case 1: f the problem size reduces to a subproblem size that is strictly smaller, and the work done at each level is essentially the same (often expressed as  af(n/b)), then the time complexity is 
O(f(n)).

Case 2: If the problem size reduces to a subproblem size that is strictly smaller, and the work done at each level is significantly different (often expressed as af(n/b)), but the work done is still polynomially larger than the recursive work, then the time complexity is O(f(n)logn).

Case 3: If the problem size doesn't reduce significantly (often expressed as af(n/b)), and the work done at each level is roughly the same, then the time complexity is O(n log  a).
                                                                           b
These cases help analyze and determine the time complexity of divide-and-conquer algorithms efficiently.
  

